{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10ede9850>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from MiniPAINN import MPAINNBlock, MPAINNPrediction\n",
    "import torch\n",
    "from torch.nn import Embedding, Module, Linear, SiLU, Embedding\n",
    "from torch_geometric.nn import global_add_pool\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from train import mini_train\n",
    "from utils import get_mini_dataloader, get_molecule, get_random_roto_reflection_translation, E3_transform_molecule, E3_transform_force, plot_molecules, plot_molecules_with_forces, bessel_rbf, cosine_cutoff\n",
    "\n",
    "torch.manual_seed(2002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_v_0(x):\n",
    "    return x[:int(x.shape[0]*0.75)].view(-1,3,16)[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPAINNPrediction(Module):\n",
    "    \"\"\"PAINN (Polarizable Atomic Interaction Neural Network) as described in https://arxiv.org/pdf/2102.03150, except with embedding dimension 16 instead of 128.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # activation function\n",
    "        self.act = SiLU()\n",
    "\n",
    "        # linear layers: 1 to update and 1 to consolidate\n",
    "        self.linear_1 = Linear(16, 16)\n",
    "        self.linear_2 = Linear(16, 1)\n",
    "        \n",
    "    def forward(self, x, data, pos):\n",
    "        # prediction head takes in only invariant embeddings, which in this architecture are organized into last 1/4 of x tensor\n",
    "        s = x[:, int(x.shape[1]*0.75):]\n",
    "        \n",
    "        # update embeddings\n",
    "        s = self.linear_1(s)\n",
    "        # nonlinearity\n",
    "        s = self.act(s)\n",
    "        # consolidate embeddings\n",
    "        s = self.linear_2(s)\n",
    "        \n",
    "        # energy prediction is sum over node values\n",
    "        E_hat = global_add_pool(s, data.batch)\n",
    "        \n",
    "        # force prediction is opposite gradient of energy with resepct to position\n",
    "        F_hat = -torch.autograd.grad(E_hat.sum(), pos, retain_graph=True)[0]\n",
    "        \n",
    "        # return energy and force predictions\n",
    "        return E_hat, F_hat\n",
    "\n",
    "class MPAINNMessage(MessagePassing):\n",
    "    \"\"\"message block, as in Figure 2b of https://arxiv.org/pdf/2102.03150.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # activation function\n",
    "        self.act = SiLU()\n",
    "        \n",
    "        # `W• + b` layers of Figure 2b, proceeding top-to-bottom\n",
    "        self.linear_1 = Linear(16,16)\n",
    "        self.linear_2 = Linear(20,16*3)\n",
    "        self.linear_3 = Linear(16,16*3)\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr1, edge_attr2):\n",
    "        # message passing to update x, embedding vector\n",
    "        x = self.propagate(edge_index=edge_index, edge_attr1=edge_attr1, edge_attr2=edge_attr2, x=x)\n",
    "\n",
    "        # return updated embedding\n",
    "        return x\n",
    "        \n",
    "    def message(self, x_j, edge_index, edge_attr1, edge_attr2):\n",
    "        # x is [(batch_size * num_nodes) x emb_dim x 4] tensor\n",
    "        # v is [(batch_size * num_nodes) x emb_dim x 3] tensor, which is unrolled into a (batch_size * num_nodes) x (emb_dim * 3) tensor for storage in x\n",
    "        # s is [(batch_size * sum_nodes) x emb_dim] tensor\n",
    "        v_j = x_j[:,:int(x_j.shape[1] * 0.75)]\n",
    "        s_j = x_j[:,int(x_j.shape[1] * 0.75):]\n",
    "        \n",
    "        # update s_j and expand it\n",
    "        s_j = self.linear_1(s_j)\n",
    "        s_j = self.act(s_j)\n",
    "        s_j = self.linear_3(s_j)\n",
    "        \n",
    "        # edge_attr1 contains normalized edge vectors\n",
    "        unit_edge_vec = edge_attr1\n",
    "        # edge_attr2 contains magnitudes of edge vectors\n",
    "        # unsqueeze so it plays nicely with bessel_rbf\n",
    "        edge_vec_length = edge_attr2.unsqueeze(dim=1)\n",
    "        \n",
    "        # write edge vec lengths in bessel rbf under cosine cutoff\n",
    "        edge_basis = bessel_rbf(x=edge_vec_length, n=20, r_cut=1.4415) * cosine_cutoff(x=edge_vec_length, r_cut=1.4415)\n",
    "        # expand via linear layer\n",
    "        edge_basis = self.linear_2(edge_basis)\n",
    "        \n",
    "        # elementwise multiply s_j and edge_basis to obtain tensor at `split` block in Figure 2b\n",
    "        split = s_j * edge_basis\n",
    "        # split this tensor into 3 equal parts\n",
    "        even_third = int(split.shape[1]/3)\n",
    "        split_1 = split[:, :even_third]\n",
    "        split_2 = split[:, even_third:-even_third]\n",
    "        split_3 = split[:, -even_third:]\n",
    "        \n",
    "        # elementwise multiply first 1/3 with equivariant embedding\n",
    "        split_1 = split_1.view(-1,1,16)\n",
    "        v_j = v_j.view(-1,3,16)\n",
    "        v_j = split_1 * v_j\n",
    "        \n",
    "        # add last 1/3 to equivariant embedding\n",
    "        # v_j += torch.einsum('ni,nj->nij', split_3, unit_edge_vec)\n",
    "        \n",
    "        # middle 1/3 is new invariant embedding\n",
    "        s_j = split_2\n",
    "        \n",
    "        # concatenate v_j and s_j, which are residuals corresponding to node j, into a single tensor of dimension [batch_size x (emb_dim * 4)]\n",
    "        out = torch.cat((v_j.view(v_j.size(0), -1), s_j), dim=1)\n",
    "        \n",
    "        # return message\n",
    "        return out\n",
    "        \n",
    "    def update(self, aggr_out, x):\n",
    "        # aggr_out contains sum of all messages\n",
    "        # new embedding is found simply by adding message to previous embedding\n",
    "        x += aggr_out\n",
    "        \n",
    "        # return new embedding\n",
    "        return x\n",
    "\n",
    "class MPAINNUpdate(MessagePassing):\n",
    "    \"\"\"update block, as in Figure 2c of https://arxiv.org/pdf/2102.03150.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # activation function\n",
    "        self.act = SiLU()\n",
    "        \n",
    "        # linear layers with bias are not equivariant\n",
    "        self.U = Linear(16,16,bias=False)\n",
    "        self.V = Linear(16,16,bias=False)\n",
    "        \n",
    "        # linear layers with bias for invariant embeddings\n",
    "        self.linear_1 = Linear(16*2,16)\n",
    "        self.linear_2 = Linear(16,16*3)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # message passing to update x, embedding vector\n",
    "        x = self.propagate(edge_index=edge_index, x=x)\n",
    "        \n",
    "        # return updated embedding\n",
    "        return x\n",
    "        \n",
    "    def message(self, x_j):\n",
    "        # again: first 3/4 are unrolled equivariant embeddings, and last 1/4 is invariant embeddings\n",
    "        v = x_j[:,:int(x_j.shape[1] * 0.75)]\n",
    "        s = x_j[:,int(x_j.shape[1] * 0.75):]\n",
    "        \n",
    "        # change dimensions of v to allow it to be put through linear layer\n",
    "        v = v.view(v.shape[0],3,16)\n",
    "        v = self.U(v)\n",
    "        \n",
    "        # v_V is tensor in second column from left in Figure 2c\n",
    "        v_V = self.V(v)\n",
    "        v_V = v_V.view(v_V.shape[0],16,3)\n",
    "        \n",
    "        # stack_in is tensor in column 2.5 from left in Figure 2c, immediately after ||•||\n",
    "        stack_in = torch.norm(v_V, p=2, dim=2)\n",
    "        \n",
    "        # concatenate stack_in and s\n",
    "        stack = torch.cat((stack_in, s), dim=1)\n",
    "        # s updated via linear layer\n",
    "        s = self.linear_1(stack)\n",
    "        # nonlinearity\n",
    "        s = self.act(s)\n",
    "        # put s through another linear layer to obtain tensor at `split` block in Figure 2c\n",
    "        split = self.linear_2(s)\n",
    "        \n",
    "        # split this tensor into 3 equal parts\n",
    "        even_third = int(split.shape[1]/3)\n",
    "        split_1 = split[:,:even_third]    \n",
    "        split_2 = split[:,even_third:-even_third]\n",
    "        split_3 = split[:,-even_third:]\n",
    "        \n",
    "        # first 1/3 is multiplied by equivariant tensor, with each element in split_1 acting on corresponding 3-dimensional vector in v\n",
    "        v = v.view(v.shape[0],3,16)\n",
    "        v = split_1.unsqueeze(dim=1) * v\n",
    "        \n",
    "        # weird ensum thing for <•_1, •_2> when batch dimensions are in play\n",
    "        # gives tensor [batch_dim x emb_dim]\n",
    "        v_V = torch.einsum('ijk,ijk->ij', v, v_V)\n",
    "        # elementiwse multiply by split_2\n",
    "        v_v = v_V * split_2\n",
    "        # add split_3\n",
    "        v_V = v_V + split_3\n",
    "        \n",
    "        # reshape v for concatenation with v_V\n",
    "        v = v.view(v.shape[0], -1)\n",
    "        \n",
    "        # v_V is s residual\n",
    "        x = torch.cat((v, v_V), dim=1)\n",
    "        \n",
    "        # return residual\n",
    "        return x\n",
    "    \n",
    "    def update(self, aggr_out, x):\n",
    "        # aggr_out contains sum of all messages\n",
    "        # new embedding is found simply by adding message to previous embedding, \n",
    "        # this is as far as I can tell: I believe Figure 2c is missing summations at bottom\n",
    "        # otherwise I do not understand how v_j and s_j become \\delta v_i^u and \\delta s_i^u\n",
    "        x += aggr_out\n",
    "    \n",
    "        # return new embedding\n",
    "        return x\n",
    "\n",
    "class MPAINNBlock(Module):\n",
    "    \"\"\"single message/update round, combining Figure 2b and Figure 2c in https://arxiv.org/pdf/2102.03150 for concision.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # initialize constituent blocks\n",
    "        self.message = MPAINNMessage()\n",
    "        self.update = MPAINNUpdate()\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr1, edge_attr2):\n",
    "        # call message passing functions\n",
    "        x = self.message(x, edge_index, edge_attr1, edge_attr2)\n",
    "        x = self.update(x, edge_index)\n",
    "        \n",
    "        # return updated embedding\n",
    "        return x\n",
    "\n",
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # embedding takes place outside all blocks\n",
    "        self.embedding = Embedding(118,16)\n",
    "        # 2 message/update rounds\n",
    "        self.block_1 = MPAINNBlock()\n",
    "        self.block_2 = MPAINNBlock()\n",
    "        # s goes through prediction head to give atomwise energy predictions, which are summed to give energy prediction for whole system\n",
    "        self.prediction = MPAINNPrediction()\n",
    "        \n",
    "    def forward(self, data):\n",
    "        # get relevant parts from data\n",
    "        edge_index = data.edge_index\n",
    "        pos = data.pos\n",
    "        # keep track of gradient of output wrt pos for force calculations\n",
    "        pos.requires_grad_(True)\n",
    "        \n",
    "        # this cannot be done in preprocessing, otherwise force would not be able to be found as negative gradient of energy\n",
    "        idx1 = edge_index[0]\n",
    "        idx2 = edge_index[1]\n",
    "        edge_vec = data.pos[idx1] - data.pos[idx2]\n",
    "        edge_vec_length = torch.norm(edge_vec, dim=1)\n",
    "        unit_edge_vec = torch.div(edge_vec, edge_vec_length.unsqueeze(dim=1))\n",
    "        \n",
    "        # initialize equivariant features as 0 vector and invariant features via embedding block\n",
    "        v = make_v0(data)\n",
    "        s = self.embedding(data.z)\n",
    "        \n",
    "        # construct x by unrolling equivariant features and concatenating invariant features\n",
    "        x = torch.cat((v.reshape(v.shape[0], -1), s), dim=1)\n",
    "        \n",
    "        # 3 message/update rounds\n",
    "        x = self.block_1(x=x, edge_index=edge_index, edge_attr1=unit_edge_vec, edge_attr2=edge_vec_length)\n",
    "        x = self.block_2(x=x, edge_index=edge_index, edge_attr1=unit_edge_vec, edge_attr2=edge_vec_length)\n",
    "        \n",
    "        # get predictions from prediction head\n",
    "        F_hat, E_hat = self.prediction(x, data, pos)\n",
    "        \n",
    "        # return for loss calculation\n",
    "        return F_hat, E_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_v0(data):\n",
    "    pos = data.pos\n",
    "    edge_index = data.edge_index\n",
    "    \n",
    "    idx1, idx2 = edge_index\n",
    "    \n",
    "    edge_vectors = pos[idx2] - pos[idx1]\n",
    "    \n",
    "    num_nodes = pos.size(0)\n",
    "    edge_vector_sums = torch.zeros((num_nodes, 3), dtype=pos.dtype)\n",
    "    \n",
    "    edge_vector_sums = edge_vector_sums.scatter_add(0, idx1.unsqueeze(1).expand(-1, 3), edge_vectors)\n",
    "    \n",
    "    edge_vector_sums = edge_vector_sums.unsqueeze(2).expand(-1,-1,16)\n",
    "    \n",
    "    return edge_vector_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 3, 16])\n",
      "torch.Size([12, 3, 16])\n"
     ]
    }
   ],
   "source": [
    "x = make_v0(molecule)\n",
    "y = make_v0(transformed_molecule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random roto-reflection:\n",
      "\t 0.136  0.708  0.693\n",
      "\t 0.830 -0.463  0.311\n",
      "\t-0.541 -0.533  0.651\n",
      "Random translation:\n",
      "\t   0.0\n",
      "\t   0.0\n",
      "\t   0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/GDL/lib/python3.11/site-packages/torch_geometric/data/dataset.py:239: UserWarning: The `pre_transform` argument differs from the one used in the pre-processed version of this dataset. If you want to make use of another pre-processing technique, pass `force_reload=True` explicitly to reload the dataset.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "roto_reflection_translation = get_random_roto_reflection_translation()\n",
    "\n",
    "molecule = get_molecule(type='benzene')\n",
    "transformed_molecule = E3_transform_molecule(molecule=molecule, roto_reflection_translation=roto_reflection_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): subscript j has size 16 for operand 1 which does not broadcast with previously seen size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmolecule\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(model(transformed_molecule))\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/GDL/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/GDL/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[60], line 249\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    246\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((v\u001b[38;5;241m.\u001b[39mreshape(v\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), s), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# 3 message/update rounds\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munit_edge_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_vec_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_2(x\u001b[38;5;241m=\u001b[39mx, edge_index\u001b[38;5;241m=\u001b[39medge_index, edge_attr1\u001b[38;5;241m=\u001b[39munit_edge_vec, edge_attr2\u001b[38;5;241m=\u001b[39medge_vec_length)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# get predictions from prediction head\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/GDL/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/GDL/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[60], line 210\u001b[0m, in \u001b[0;36mMPAINNBlock.forward\u001b[0;34m(self, x, edge_index, edge_attr1, edge_attr2)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index, edge_attr1, edge_attr2):\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;66;03m# call message passing functions\u001b[39;00m\n\u001b[1;32m    209\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessage(x, edge_index, edge_attr1, edge_attr2)\n\u001b[0;32m--> 210\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;66;03m# return updated embedding\u001b[39;00m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/GDL/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/GDL/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[60], line 130\u001b[0m, in \u001b[0;36mMPAINNUpdate.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index):\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;66;03m# message passing to update x, embedding vector\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# return updated embedding\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/GDL/lib/python3.11/site-packages/torch_geometric/nn/conv/message_passing.py:547\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    546\u001b[0m         msg_kwargs \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m res\n\u001b[0;32m--> 547\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmsg_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_forward_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    549\u001b[0m     res \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, (msg_kwargs, ), out)\n",
      "Cell \u001b[0;32mIn[60], line 172\u001b[0m, in \u001b[0;36mMPAINNUpdate.message\u001b[0;34m(self, x_j)\u001b[0m\n\u001b[1;32m    168\u001b[0m v \u001b[38;5;241m=\u001b[39m split_1\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m v\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# weird ensum thing for <•_1, •_2> when batch dimensions are in play\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# gives tensor [batch_dim x emb_dim]\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m v_V \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mijk,ijk->ij\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_V\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# elementiwse multiply by split_2\u001b[39;00m\n\u001b[1;32m    174\u001b[0m v_v \u001b[38;5;241m=\u001b[39m v_V \u001b[38;5;241m*\u001b[39m split_2\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/GDL/lib/python3.11/site-packages/torch/functional.py:377\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    379\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mRuntimeError\u001b[0m: einsum(): subscript j has size 16 for operand 1 which does not broadcast with previously seen size 3"
     ]
    }
   ],
   "source": [
    "print(model(molecule))\n",
    "print(model(transformed_molecule))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7365,  0.3569, -0.3672],\n",
       "        [ 0.9887, -1.2250, -0.6037],\n",
       "        [-0.6021, -1.5520, -0.2673],\n",
       "        [-1.7365, -0.3569,  0.3672],\n",
       "        [-0.9887,  1.2250,  0.6037],\n",
       "        [ 0.6021,  1.5520,  0.2673],\n",
       "        [-1.3042, -0.2681,  0.2758],\n",
       "        [-0.7691,  0.9213,  0.4609],\n",
       "        [ 0.4770,  1.1774,  0.1973],\n",
       "        [ 1.3042,  0.2681, -0.2758],\n",
       "        [ 0.7691, -0.9213, -0.4609],\n",
       "        [-0.4770, -1.1774, -0.1973]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(roto_reflection_translation[0], torch.tensor([[-0.0000,  1.8104, -0.0000],\n",
    "        [ 1.4680,  0.8293, -0.0000],\n",
    "        [ 1.4680, -0.8293, -0.0000],\n",
    "        [-0.0000, -1.8104, -0.0000],\n",
    "        [-1.4680, -0.8293, -0.0000],\n",
    "        [-1.4680,  0.8293, -0.0000],\n",
    "        [-0.0000, -1.3597, -0.0000],\n",
    "        [-1.1094, -0.6496, -0.0000],\n",
    "        [-1.1094,  0.6496, -0.0000],\n",
    "        [-0.0000,  1.3597, -0.0000],\n",
    "        [ 1.1094,  0.6496, -0.0000],\n",
    "        [ 1.1094, -0.6496, -0.0000]]).transpose(0,1)).transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 3, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2420, -0.2420, -0.2420, -0.2420, -0.2420, -0.2420, -0.2420, -0.2420,\n",
       "         -0.2420, -0.2420, -0.2420, -0.2420, -0.2420, -0.2420, -0.2420, -0.2420],\n",
       "        [-0.0497, -0.0497, -0.0497, -0.0497, -0.0497, -0.0497, -0.0497, -0.0497,\n",
       "         -0.0497, -0.0497, -0.0497, -0.0497, -0.0497, -0.0497, -0.0497, -0.0497],\n",
       "        [ 0.0512,  0.0512,  0.0512,  0.0512,  0.0512,  0.0512,  0.0512,  0.0512,\n",
       "          0.0512,  0.0512,  0.0512,  0.0512,  0.0512,  0.0512,  0.0512,  0.0512]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_v0(transformed_molecule)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
